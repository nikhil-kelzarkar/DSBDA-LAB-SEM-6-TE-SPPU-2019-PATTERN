{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92041d47-9b20-49dc-9965-d0146c16cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f4f9ec-9109-472c-aad7-32f4dddbac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords): \n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords) \n",
    "    for word, count in wordDict.items():\n",
    "         tfDict[word] = count / float(bagOfWordsCount) \n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c574fe-ce92-4051-8b1b-cada2c600b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0) \n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "                for word, val in idfDict.items():\n",
    "                    if val > 0: idfDict[word] = math.log(N / float(val))\n",
    "                    else: idfDict[word] = 0\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a2e46d-f2e0-41a5-9488-d7b2b1cf579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given sentences are: \n",
      " Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs): \n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items(): \n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n",
    "print('The given sentences are: \\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765c75f6-36cc-4f92-8020-f88f342a178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sentence Tokenization: \n",
      " ['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "tokenized_text= sent_tokenize(text) \n",
    "print(\"\\n Sentence Tokenization: \\n\", tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ab32eb-0b24-4e84-b00e-e58427a75e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Tokeniztion: \n",
      " ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "tokenized_word=word_tokenize(text) \n",
    "print('\\nWord Tokeniztion: \\n', tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eebcec3e-7b13-4088-a73d-3e27cc97d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a062572f-747a-4d5b-95d9-f898d655b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words=set(stopwords.words(\"english\")) \n",
    "# Removing stop words\n",
    "text= \"How to remove stop words with NLTK library in Python?\" \n",
    "text= re.sub('[^a-zA-Z]', ' ',text) \n",
    "tokens = word_tokenize(text.lower()) \n",
    "filtered_text=[] \n",
    "for w in tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_text.append(w) \n",
    "print (\"Tokenized Sentence:\",tokens) \n",
    "print (\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aaf3c5b-30cf-4a29-a2a8-76132cb49a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for  wait :  wait\n",
      "Stemming for  waiting :  wait\n",
      "Stemming for  waited :  wait\n",
      "Stemming for  waits :  wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"] \n",
    "ps =PorterStemmer() \n",
    "for w in e_words: \n",
    "    rootWord=ps.stem(w) \n",
    "    print('Stemming for ',w,': ',rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7920350-22dd-45f7-a4e7-d50858f2601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "text = \"studies studying cries cry\" \n",
    "tokenization = nltk.word_tokenize(text) \n",
    "for w in tokenization: \n",
    "    print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffe0e1-b60d-414b-b782-e6088976e6cf",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89063d96-a673-48e5-b5f9-3b5f8fb47f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092d7a40-01ff-47f4-98d8-ee19291f79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'Jupiter is the largest planet' \n",
    "documentB = 'Mars is the fourth planet from the Sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6309ee42-a772-439f-b954-6fa55f18c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ') \n",
    "bagOfWordsB = documentB.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6fe0bd4-666d-442f-a2cb-2cf93cd2a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9e971a2-dac0-476f-a119-00916de28aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "    numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd44b0a7-6f7e-489f-8f63-602abed207c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = computeTF(numOfWordsA, bagOfWordsA) \n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "227c32fd-8c64-4d4d-ba9d-a59d81a095e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Term Frequency----------------------\n",
      "   planet     is  Jupiter   Mars    Sun   the  largest  fourth   from\n",
      "0   0.200  0.200      0.2  0.000  0.000  0.20      0.2   0.000  0.000\n",
      "1   0.125  0.125      0.0  0.125  0.125  0.25      0.0   0.125  0.125\n"
     ]
    }
   ],
   "source": [
    "print('----------------Term Frequency----------------------')\n",
    "df = pd.DataFrame([tfA, tfB]) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baa15539-5ec9-4ee0-987d-fb9e5b182d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Inverse Document Frequency----------------------\n",
      "{'planet': 0, 'is': 0, 'Jupiter': 1.9392430739608337, 'Mars': 0.5560285979740467, 'Sun': 1.1469641504314008, 'the': 0, 'largest': 1.5000848184268372, 'fourth': 1.0596601011416096, 'from': 0.6931471805599453}\n",
      "------------------- TF-IDF--------------------------------------\n",
      "   planet   is   Jupiter      Mars       Sun  the   largest    fourth  \\\n",
      "0     0.0  0.0  0.387849  0.000000  0.000000  0.0  0.300017  0.000000   \n",
      "1     0.0  0.0  0.000000  0.069504  0.143371  0.0  0.000000  0.132458   \n",
      "\n",
      "       from  \n",
      "0  0.000000  \n",
      "1  0.086643  \n"
     ]
    }
   ],
   "source": [
    "idfs = computeIDF([numOfWordsA, numOfWordsB]) \n",
    "print('----------------Inverse Document Frequency----------------------')\n",
    "print(idfs)\n",
    "tfidfA = computeTFIDF(tfA, idfs) \n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "print('------------------- TF-IDF--------------------------------------')\n",
    "df = pd.DataFrame([tfidfA, tfidfB]) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfdfe1-aa2d-414a-89bc-c6dacf6d6d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
